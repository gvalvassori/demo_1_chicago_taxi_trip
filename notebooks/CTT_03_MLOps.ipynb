{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<h1 style=\"font-family:verdana;\"> Description:</h1>\n",
    "\n",
    "<ul>\n",
    "<li><p style=\"font-family:verdana;\">\n",
    "In this notebook, we are going to develop the complete MLOps pipeline for the Chicago Taxi trip use case, using Tensorflow Extended and Vertex AI.\n",
    "</p></li>\n",
    "\n",
    "<li><p style=\"font-family:verdana;\">\n",
    "Data pre-processing is performed with BigQuery.\n",
    "</p></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Import needed packages](#step-0-import-needed-packages)\n",
    "2. [Set up GCP environment variables](#step-1-set-up-gcp-environment-parameters)\n",
    "3. [Data extraction and pre-processing with BigQuery](#step-2-data-extraction-from-bigquery-with-pre-processing)\n",
    "4. [TensorFlow model trainer code](#step-3-model-trainer)\n",
    "5. [TensorFlow eXtended pipeline](#step-4-create-pipeline)\n",
    "6. [Execute pipeline on Vertex AI Pipelines](#step-5-execute-pipeline-on-vertex-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up GCP environment parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCP Project params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'demoespecialidadgcp'\n",
    "GOOGLE_CLOUD_PROJECT_NUMBER = '502688298240'\n",
    "GOOGLE_CLOUD_REGION = 'us-east1'\n",
    "GCS_BUCKET_NAME = 'demo-1-chicago-taxi-fare'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex AI Pipeline params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://demo-1-chicago-taxi-fare/pipeline_root/demo-1-pipeline-v3\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = 'demo-1-pipeline-v3'\n",
    "ENDPOINT_NAME = 'demo-1-model-v3'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' data.\n",
    "DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://demo-1-chicago-taxi-fare/pipeline_module/demo-1-pipeline-v3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data extraction from BigQuery with pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "        SELECT \n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            trip_total,\n",
    "            CASE WHEN EXTRACT(DAYOFWEEK FROM trip_start_timestamp) < 5 THEN 1 ELSE 0 END AS work_day,\n",
    "            CASE WHEN EXTRACT(HOUR FROM trip_start_timestamp) BETWEEN 8 AND 18 THEN 1 ELSE 0 END AS work_hour,\n",
    "            trip_miles/(trip_seconds/3600) AS trip_speed,\n",
    "            pickup_community_area,\n",
    "            dropoff_community_area\n",
    "        FROM \n",
    "            `demoespecialidadgcp.demo_1.training_dataset`\n",
    "        WHERE\n",
    "            trip_seconds > 0 AND trip_seconds < 7200 AND\n",
    "            trip_miles > 0 AND\n",
    "            trip_total > 0 AND trip_total < 250 AND\n",
    "            payment_type = \"Cash\" AND payment_type = \"Cash\" AND\n",
    "            trip_start_timestamp IS NOT NULL AND\n",
    "            pickup_community_area IS NOT NULL AND\n",
    "            dropoff_community_area IS NOT NULL AND\n",
    "            RAND() < 2000000/(SELECT COUNT(*) FROM `demoespecialidadgcp.demo_1.training_dataset`)\n",
    "        LIMIT 200000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chicago_taxi_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chicago_taxi_trainer.py\n",
    "\n",
    "from typing import List, Text\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "import keras_tuner\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "# Specify features that we will use.\n",
    "_FEATURE_KEYS = [\n",
    "    \"trip_seconds\", \"trip_miles\", \"work_day\", \"work_hour\",\n",
    "    \"trip_speed\", \"pickup_community_area\", \"dropoff_community_area\"\n",
    "]\n",
    "\n",
    "_LABEL_KEY = \"trip_total\"\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 64\n",
    "_EVAL_BATCH_SIZE = 32\n",
    "\n",
    "# NEW: TFX Transform will call this function.\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "    Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "    Returns:\n",
    "    Map from string feature key to transformed feature.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Uses features defined in _FEATURE_KEYS only.\n",
    "    for key in _FEATURE_KEYS:\n",
    "        if key in [\"work_day\", \"work_hour\"]:\n",
    "            # Convert work_day and work_hour to float32\n",
    "            outputs[key] = tf.cast(inputs[key], tf.float32)\n",
    "        else:\n",
    "            # For other features, apply z-score normalization\n",
    "            outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "    \n",
    "    outputs[_LABEL_KEY] = inputs[_LABEL_KEY]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "# NEW: This function will create a handler function which gets a serialized\n",
    "#      tf.example, preprocess and run an inference with it.\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    # We must save the tft_layer to the model to ensure its assets are kept and\n",
    "    # tracked.\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "                # Expected input is a string which is serialized tf.Example format.\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        # Remove label feature since these will not be present at serving time.\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
    "                                              feature_spec)\n",
    "\n",
    "        # Preprocess parsed input with transform operation defined in\n",
    "        # preprocessing_fn().\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        # Run inference with ML model.\n",
    "        outputs = model(transformed_features)\n",
    "        \n",
    "        return {'predictions': outputs}        \n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _get_serve_raw_examples_fn(model, tf_transform_output):\n",
    "    # This function will create the custom serving signature\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_raw_examples_fn(trip_seconds, \n",
    "                              trip_miles, \n",
    "                              work_day, \n",
    "                              work_hour,\n",
    "                              trip_speed, \n",
    "                              pickup_community_area, \n",
    "                              dropoff_community_area):\n",
    "        features = {\n",
    "            \"trip_seconds\": trip_seconds, \n",
    "            \"trip_miles\": trip_miles, \n",
    "            \"work_day\": tf.cast(work_day, tf.float32), \n",
    "            \"work_hour\": tf.cast(work_hour, tf.float32),\n",
    "            \"trip_speed\": trip_speed, \n",
    "            \"pickup_community_area\": tf.cast(pickup_community_area, tf.float32), \n",
    "            \"dropoff_community_area\": tf.cast(dropoff_community_area, tf.float32)\n",
    "        }\n",
    "              \n",
    "        transformed_features = model.tft_layer(features)\n",
    "        outputs = model(transformed_features)\n",
    "        \n",
    "        return {'predictions': outputs}\n",
    "\n",
    "    return serve_raw_examples_fn\n",
    "\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 64) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "   \n",
    "    dataset = data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        tf_transform_output.transformed_metadata.schema)\n",
    "    dataset=dataset.repeat()\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def _build_keras_model(hparams: keras_tuner.HyperParameters) -> tf.keras.Model:\n",
    "    \"\"\"Builds a Keras model for taxi trip fare prediction with hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        hparams: (Hyperparameter) Hyperparameter object for tuning.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    units_1 = hparams.get('units_1')\n",
    "    units_2 = hparams.get('units_2')\n",
    "    learning_rate = hparams.get('learning_rate')\n",
    "    activation = \"relu\"\n",
    "    dropout_rate = hparams.get('dropout_rate')\n",
    "    l2_regularization = hparams.get('l2_regularization')\n",
    "\n",
    "    # Create inputs for each feature\n",
    "    inputs = {\n",
    "        feature: tf.keras.Input(shape=(1,), name=feature, dtype=tf.float32)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    }\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concat = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
    "\n",
    "    # Add first hidden layer with dropout and L2 regularization\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(\n",
    "        units_1, \n",
    "        activation=activation,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)\n",
    "    )(concat)\n",
    "    hidden_layer_1 = tf.keras.layers.Dropout(dropout_rate)(hidden_layer_1)\n",
    "\n",
    "    # Add second hidden layer with dropout and L2 regularization\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(\n",
    "        units_2, \n",
    "        activation=activation,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)\n",
    "    )(hidden_layer_1)\n",
    "    hidden_layer_2 = tf.keras.layers.Dropout(dropout_rate)(hidden_layer_2)\n",
    "\n",
    "    # Add output layer\n",
    "    output = tf.keras.layers.Dense(1)(hidden_layer_2)\n",
    "\n",
    "    # Create the functional model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Compile the model using Adam optimizer with tuned learning rate\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='huber_loss', metrics=['mean_absolute_error'])\n",
    "    \n",
    "    model.summary(print_fn=logging.info)\n",
    "\n",
    "    return model\n",
    "\n",
    "def _get_hyperparameters() -> keras_tuner.HyperParameters:\n",
    "    \"\"\"Defines the search space for hyperparameters.\"\"\"\n",
    "    hp = keras_tuner.HyperParameters()\n",
    "    hp.Int('units_1', min_value=32, max_value=512, step=32)\n",
    "    hp.Int('units_2', min_value=16, max_value=256, step=16)\n",
    "    hp.Float('learning_rate', min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    hp.Float('l2_regularization', min_value=1e-6, max_value=1e-1, sampling='log')\n",
    "    return hp\n",
    "\n",
    "# TFX Tuner will call this function.\n",
    "def tuner_fn(fn_args: tfx.components.FnArgs) -> tfx.components.TunerFnResult:\n",
    "    \"\"\"Build the tuner using the KerasTuner API.\n",
    "\n",
    "    Args:\n",
    "    fn_args: Holds args as name/value pairs.\n",
    "      - working_dir: working dir for tuning.\n",
    "      - train_files: List of file paths containing training tf.Example data.\n",
    "      - eval_files: List of file paths containing eval tf.Example data.\n",
    "      - train_steps: number of train steps.\n",
    "      - eval_steps: number of eval steps.\n",
    "      - schema_path: optional schema of the input data.\n",
    "      - transform_graph_path: optional transform graph produced by TFT.\n",
    "\n",
    "    Returns:\n",
    "    A namedtuple contains the following:\n",
    "      - tuner: A BaseTuner that will be used for tuning.\n",
    "      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
    "                    model , e.g., the training and validation dataset. Required\n",
    "                    args depend on the above tuner's implementation.\n",
    "    \"\"\"\n",
    "    # RandomSearch is a subclass of keras_tuner.Tuner which inherits from\n",
    "    # BaseTuner.\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "      _build_keras_model,\n",
    "      max_trials=20, \n",
    "      hyperparameters=_get_hyperparameters(),\n",
    "      allow_new_entries=False,\n",
    "      objective=keras_tuner.Objective('val_mean_absolute_error', 'min'),\n",
    "      directory=fn_args.working_dir,\n",
    "      project_name='demo_1_tuning')\n",
    "\n",
    "    transform_graph = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      _TRAIN_BATCH_SIZE)\n",
    "\n",
    "    eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      transform_graph,\n",
    "      _EVAL_BATCH_SIZE)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, mode='auto')\n",
    "\n",
    "    return tfx.components.TunerFnResult(\n",
    "      tuner=tuner,\n",
    "      fit_kwargs={\n",
    "          'x': train_dataset,\n",
    "          'validation_data': eval_dataset,\n",
    "          'steps_per_epoch': fn_args.train_steps,\n",
    "          'validation_steps': fn_args.eval_steps,\n",
    "          'epochs': 16, \n",
    "          'callbacks':[early_stopping]\n",
    "      })\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        batch_size=_TRAIN_BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files,\n",
    "        fn_args.data_accessor,\n",
    "        tf_transform_output,\n",
    "        batch_size=_EVAL_BATCH_SIZE)\n",
    "    \n",
    "    if fn_args.hyperparameters:\n",
    "        hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)\n",
    "    else:\n",
    "        # This is a shown case when hyperparameters is decided and Tuner is removed\n",
    "        # from the pipeline. User can also inline the hyperparameters directly in\n",
    "        # _build_keras_model.\n",
    "        hparams = _get_hyperparameters()\n",
    "\n",
    "    model = _build_keras_model(hparams)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Write logs to path\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fn_args.model_run_dir, update_freq='epoch')\n",
    "    \"\"\"\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, mode='auto')\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping])\n",
    "    \n",
    "    # NEW: Save a computation graph including transform layer.\n",
    "    \n",
    "    signatures = {\n",
    "        'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
    "        'serving_raw_examples': _get_serve_raw_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"trip_seconds\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"trip_miles\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"work_day\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"work_hour\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"trip_speed\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"pickup_community_area\"),\n",
    "            tf.TensorSpec(shape=[None,1], dtype=tf.float32, name=\"dropoff_community_area\"),\n",
    "        )}\n",
    "\n",
    "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "    # directory.\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the trainer file to `MODULE_ROOT` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://chicago_taxi_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][ 11.8 KiB/ 11.8 KiB]                                                \n",
      "Operation completed over 1 objects/11.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Copy chicago_taxi_trainer.py to MODULE_ROOT\n",
    "!gsutil cp chicago_taxi_trainer.py $MODULE_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 15:57:47.363523: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-18 15:57:47.365153: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-18 15:57:47.393531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-18 15:57:47.393565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-18 15:57:47.394531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-18 15:57:47.399249: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-18 15:57:47.399731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-18 15:57:48.097826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, query: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, beam_pipeline_args: Optional[List[str]]) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Creates a TFX pipeline using BigQuery.\"\"\"\n",
    "\n",
    "    # Query data in BigQuery as a data source.\n",
    "    example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(\n",
    "          query=query).with_id('example_gen_demo_1')\n",
    "    \n",
    "    stats_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']).with_id('statistics_gen_demo_1')\n",
    "    \n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=stats_gen.outputs['statistics']).with_id('schema_gen_demo_1')\n",
    "    \n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        materialize=True,\n",
    "        module_file=module_file).with_id('transform_demo_1')\n",
    "    \n",
    "    # Configuration for Vertex AI Training.\n",
    "    # This dictionary will be passed as `CustomJobSpec`.\n",
    "    vertex_job_spec = {\n",
    "        'project': project_id,\n",
    "        'worker_pool_specs': [{\n",
    "            'machine_spec': {\n",
    "                'machine_type': 'n1-standard-8',\n",
    "            },\n",
    "            'replica_count': 1,\n",
    "            'container_spec': {\n",
    "                'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "            },\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    tuner = tfx.components.Tuner(\n",
    "        module_file=module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=3125),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=3125)).with_id('tuner_demo_1')\n",
    "\n",
    "    # Trains a model using Vertex AI Training.\n",
    "    trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=3125),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=3125),\n",
    "        custom_config={\n",
    "            tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "                True,\n",
    "            tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "                region,\n",
    "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "                vertex_job_spec,\n",
    "            'use_gpu':\n",
    "                False,\n",
    "      }).with_id('trainer_demo_1')\n",
    "    \n",
    "    _LABEL_KEY = \"trip_total\"\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[\n",
    "            # This assumes a serving model with signature 'serving_default'. If\n",
    "            # using estimator based EvalSavedModel, add signature_name='eval' and\n",
    "            # remove the label_key. Note, if using a TFLite model, then you must set\n",
    "            # model_type='tf_lite'.\n",
    "            tfma.ModelSpec(label_key=_LABEL_KEY)\n",
    "        ],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(\n",
    "                # The metrics added here are in addition to those saved with the\n",
    "                # model (assuming either a keras model or EvalSavedModel is used).\n",
    "                # Any metrics added into the saved model (for example using\n",
    "                # model.compile(..., metrics=[...]), etc) will be computed\n",
    "                # automatically.\n",
    "                metrics=[\n",
    "                    tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                    tfma.MetricConfig(\n",
    "                        class_name='MeanAbsoluteError',\n",
    "                        threshold=tfma.MetricThreshold(\n",
    "                            value_threshold=tfma.GenericValueThreshold(\n",
    "                                lower_bound={'value': 0.0},\n",
    "                                upper_bound={'value': 2.0}),\n",
    "                            change_threshold=tfma.GenericChangeThreshold(\n",
    "                                direction=tfma.MetricDirection.LOWER_IS_BETTER,\n",
    "                                relative={'value': 1.2})))\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        slicing_specs=[\n",
    "            # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "            tfma.SlicingSpec(),\n",
    "            # Data can be sliced along a feature column.\n",
    "        ])\n",
    "\n",
    "    # The following component is experimental and may change in the future. This is\n",
    "    # required to specify the latest blessed model will be used as the baseline.\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "          strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "          model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "          model_blessing=tfx.dsl.Channel(type=tfx.types.standard_artifacts.ModelBlessing)\n",
    "    ).with_id('latest_blessing_demo_1')\n",
    "    \n",
    "    model_analyzer = tfx.components.Evaluator(\n",
    "          examples=example_gen.outputs['examples'],\n",
    "          model=trainer.outputs['model'],\n",
    "          baseline_model=model_resolver.outputs['model'],\n",
    "          # Change threshold will be ignored if there is no baseline (first run).\n",
    "          eval_config=eval_config).with_id('evaluator_demo_1')\n",
    "    \n",
    "    # NEW: Configuration for pusher.\n",
    "    vertex_serving_spec = {\n",
    "        'project_id': project_id,\n",
    "        'endpoint_name': endpoint_name,\n",
    "        'machine_type': 'n1-standard-2',\n",
    "    }\n",
    "\n",
    "    # Vertex AI provides pre-built containers with various configurations for\n",
    "    # serving.\n",
    "    # See https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "    # for available container images.\n",
    "    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest'\n",
    "    \n",
    "    # NEW: Pushes the model to Vertex AI.\n",
    "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "            custom_config={\n",
    "                tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "                    True,\n",
    "                tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "                    region,\n",
    "                tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "                    serving_image,\n",
    "                tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "                    vertex_serving_spec,\n",
    "      }).with_id('pusher_demo_1')\n",
    "    \n",
    "    components = [\n",
    "        example_gen,\n",
    "        stats_gen,\n",
    "        schema_gen,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        model_analyzer,\n",
    "        pusher\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pipeline JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpinl8qtko\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/chicago_taxi_trainer.py -> /tmp/tmpinl8qtko\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_transform_demo_1.egg-info\n",
      "writing tfx_user_code_transform_demo_1.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_transform_demo_1.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_transform_demo_1.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_transform_demo_1.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_transform_demo_1.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_transform_demo_1.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_transform_demo_1.egg-info to /tmp/tmpinl8qtko/tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpinl8qtko/tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL\n",
      "creating '/tmp/tmp2f6b_en4/tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3-none-any.whl' and adding '/tmp/tmpinl8qtko' to it\n",
      "adding 'chicago_taxi_trainer.py'\n",
      "adding 'tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/METADATA'\n",
      "adding 'tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_transform_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/RECORD'\n",
      "removing /tmp/tmpinl8qtko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvalva/corebi/especialidad/demos_en_github/demo-1/.venv/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpr2qhimm3\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/chicago_taxi_trainer.py -> /tmp/tmpr2qhimm3\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_tuner_demo_1.egg-info\n",
      "writing tfx_user_code_tuner_demo_1.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_tuner_demo_1.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_tuner_demo_1.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_tuner_demo_1.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_tuner_demo_1.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_tuner_demo_1.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_tuner_demo_1.egg-info to /tmp/tmpr2qhimm3/tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpr2qhimm3/tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL\n",
      "creating '/tmp/tmpxha526gi/tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3-none-any.whl' and adding '/tmp/tmpr2qhimm3' to it\n",
      "adding 'chicago_taxi_trainer.py'\n",
      "adding 'tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/METADATA'\n",
      "adding 'tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_tuner_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/RECORD'\n",
      "removing /tmp/tmpr2qhimm3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvalva/corebi/especialidad/demos_en_github/demo-1/.venv/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying chicago_taxi_trainer.py -> build/lib\n",
      "installing to /tmp/tmpgub7helz\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/chicago_taxi_trainer.py -> /tmp/tmpgub7helz\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_trainer_demo_1.egg-info\n",
      "writing tfx_user_code_trainer_demo_1.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_trainer_demo_1.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_trainer_demo_1.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_trainer_demo_1.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_trainer_demo_1.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_trainer_demo_1.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_trainer_demo_1.egg-info to /tmp/tmpgub7helz/tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpgub7helz/tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL\n",
      "creating '/tmp/tmp3zt499rh/tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3-py3-none-any.whl' and adding '/tmp/tmpgub7helz' to it\n",
      "adding 'chicago_taxi_trainer.py'\n",
      "adding 'tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/METADATA'\n",
      "adding 'tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_trainer_demo_1-0.0+7bdeb48a4fd5b533b979d648c5ec8ec517763be5915017b1dc6d30197bfa54e3.dist-info/RECORD'\n",
      "removing /tmp/tmpgub7helz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvalva/corebi/especialidad/demos_en_github/demo-1/.venv/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "MODEL_VERSION_NAME = 'chicago-taxi-fare-model-v' + now.strftime(\"%d%m%Y-%H%M%S\")\n",
    "\n",
    "import os\n",
    "\n",
    "# We need to pass some GCP related configs to BigQuery. This is currently done\n",
    "# using `beam_pipeline_args` parameter.\n",
    "BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
    "   '--project=' + GOOGLE_CLOUD_PROJECT,\n",
    "   '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
    "   ]\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = f\"{PIPELINE_NAME}.json\"\n",
    "\n",
    "_trainer_module_file = 'chicago_taxi_trainer.py'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        query=QUERY,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/502688298240/locations/us-east1/pipelineJobs/demo-1-pipeline-v3-20240818155804\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/502688298240/locations/us-east1/pipelineJobs/demo-1-pipeline-v3-20240818155804')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/demo-1-pipeline-v3-20240818155804?project=502688298240\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
